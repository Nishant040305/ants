{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e630dfa",
   "metadata": {},
   "source": [
    "# MITM Proxy Log Visualization Dashboard\n",
    "\n",
    "This notebook provides comprehensive analysis and visualization of MITM proxy logs. It includes traffic pattern analysis, security monitoring, and interactive dashboards for understanding network behavior.\n",
    "\n",
    "## Features:\n",
    "- üìä HTTP Traffic Pattern Analysis\n",
    "- üîí Security-focused Visualizations\n",
    "- ‚è±Ô∏è Timing and Performance Analysis  \n",
    "- üåê Host and Domain Intelligence\n",
    "- üìà Interactive Dashboards\n",
    "- üîç Anomaly Detection\n",
    "- üìã Export and Reporting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19160aa",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4223ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Interactive widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# URL and domain analysis\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set style preferences\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8d53c",
   "metadata": {},
   "source": [
    "## 2. Load and Parse MITM Log Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71380c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mitm_logs(log_path=\"../data_extraction/logs/mitm_logs/\"):\n",
    "    \"\"\"\n",
    "    Load and parse MITM proxy logs from JSONL files\n",
    "    \"\"\"\n",
    "    log_dir = Path(log_path)\n",
    "    all_logs = []\n",
    "    \n",
    "    print(f\"üîç Searching for log files in: {log_dir}\")\n",
    "    \n",
    "    # Find all JSONL files\n",
    "    jsonl_files = list(log_dir.glob(\"*.jsonl\"))\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        print(\"‚ùå No JSONL files found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"üìÅ Found {len(jsonl_files)} log files:\")\n",
    "    \n",
    "    for file_path in jsonl_files:\n",
    "        print(f\"  üìÑ Loading: {file_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    if line.strip():\n",
    "                        try:\n",
    "                            log_entry = json.loads(line.strip())\n",
    "                            log_entry['source_file'] = file_path.name\n",
    "                            all_logs.append(log_entry)\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"    ‚ö†Ô∏è  Line {line_num}: JSON decode error - {e}\")\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Error reading {file_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Loaded {len(all_logs)} log entries total\")\n",
    "    \n",
    "    if not all_logs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_logs)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the logs\n",
    "df_raw = load_mitm_logs()\n",
    "\n",
    "if not df_raw.empty:\n",
    "    print(f\"\\nüìä Dataset Overview:\")\n",
    "    print(f\"   ‚Ä¢ Total entries: {len(df_raw):,}\")\n",
    "    print(f\"   ‚Ä¢ Columns: {list(df_raw.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Memory usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data loaded. Please check the log file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a81b2",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mitm_data(df_raw):\n",
    "    \"\"\"\n",
    "    Clean and structure MITM log data for analysis\n",
    "    \"\"\"\n",
    "    if df_raw.empty:\n",
    "        print(\"‚ùå No data to process\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(\"üîß Preprocessing MITM log data...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # Parse timestamp\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    \n",
    "    # Extract request information\n",
    "    if 'request' in df.columns:\n",
    "        df['method'] = df['request'].apply(lambda x: x.get('method', '') if isinstance(x, dict) else '')\n",
    "        df['host'] = df['request'].apply(lambda x: x.get('host', '') if isinstance(x, dict) else '')\n",
    "        df['path'] = df['request'].apply(lambda x: x.get('path', '') if isinstance(x, dict) else '')\n",
    "        df['scheme'] = df['request'].apply(lambda x: x.get('scheme', '') if isinstance(x, dict) else '')\n",
    "        df['port'] = df['request'].apply(lambda x: x.get('port', 0) if isinstance(x, dict) else 0)\n",
    "        df['user_agent'] = df['request'].apply(lambda x: x.get('headers', {}).get('user-agent', '') if isinstance(x, dict) else '')\n",
    "        df['req_content_length'] = df['request'].apply(lambda x: int(x.get('headers', {}).get('content-length', 0) or 0) if isinstance(x, dict) else 0)\n",
    "    \n",
    "    # Extract response information\n",
    "    if 'response' in df.columns:\n",
    "        df['status_code'] = df['response'].apply(lambda x: x.get('status_code', 0) if isinstance(x, dict) else 0)\n",
    "        df['response_reason'] = df['response'].apply(lambda x: x.get('reason', '') if isinstance(x, dict) else '')\n",
    "        df['resp_content_length'] = df['response'].apply(lambda x: int(x.get('headers', {}).get('content-length', 0) or 0) if isinstance(x, dict) else 0)\n",
    "        df['content_type'] = df['response'].apply(lambda x: x.get('headers', {}).get('content-type', '') if isinstance(x, dict) else '')\n",
    "    \n",
    "    # Extract timing information\n",
    "    if 'timings' in df.columns:\n",
    "        df['request_duration'] = df['timings'].apply(lambda x: \n",
    "            (x.get('response_end', 0) - x.get('request_start', 0)) * 1000 \n",
    "            if isinstance(x, dict) and x.get('request_start') and x.get('response_end') \n",
    "            else 0)\n",
    "    \n",
    "    # Extract domain and TLD information\n",
    "    df['domain'] = df['host'].apply(lambda x: tldextract.extract(x).domain if x else '')\n",
    "    df['tld'] = df['host'].apply(lambda x: tldextract.extract(x).suffix if x else '')\n",
    "    df['subdomain'] = df['host'].apply(lambda x: tldextract.extract(x).subdomain if x else '')\n",
    "    \n",
    "    # Create full URL\n",
    "    df['full_url'] = df.apply(lambda row: f\"{row['scheme']}://{row['host']}{row['path']}\" if row['host'] else '', axis=1)\n",
    "    \n",
    "    # Categorize HTTP methods\n",
    "    df['method_category'] = df['method'].apply(lambda x: \n",
    "        'READ' if x in ['GET', 'HEAD'] else\n",
    "        'WRITE' if x in ['POST', 'PUT', 'PATCH'] else\n",
    "        'DELETE' if x == 'DELETE' else\n",
    "        'OTHER')\n",
    "    \n",
    "    # Categorize status codes\n",
    "    df['status_category'] = df['status_code'].apply(lambda x:\n",
    "        'Success' if 200 <= x < 300 else\n",
    "        'Redirect' if 300 <= x < 400 else\n",
    "        'Client Error' if 400 <= x < 500 else\n",
    "        'Server Error' if 500 <= x < 600 else\n",
    "        'Unknown')\n",
    "    \n",
    "    # Identify potentially suspicious patterns\n",
    "    df['is_api_call'] = df['path'].str.contains(r'/api/|/v\\d+/|\\.json|\\.xml', case=False, na=False)\n",
    "    df['has_query_params'] = df['path'].str.contains(r'\\?', na=False)\n",
    "    df['is_secure'] = df['scheme'] == 'https'\n",
    "    df['is_large_request'] = df['req_content_length'] > 1000000  # >1MB\n",
    "    df['is_large_response'] = df['resp_content_length'] > 1000000  # >1MB\n",
    "    \n",
    "    # Time-based features\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    \n",
    "    # Drop rows with invalid timestamps\n",
    "    df = df.dropna(subset=['datetime'])\n",
    "    \n",
    "    print(f\"‚úÖ Preprocessing complete!\")\n",
    "    print(f\"   ‚Ä¢ Final dataset size: {len(df):,} entries\")\n",
    "    print(f\"   ‚Ä¢ Time range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "    print(f\"   ‚Ä¢ Unique hosts: {df['host'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ HTTP methods: {df['method'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process the data\n",
    "df = preprocess_mitm_data(df_raw)\n",
    "\n",
    "# Display sample data\n",
    "if not df.empty:\n",
    "    print(f\"\\nüìã Sample processed data:\")\n",
    "    display(df[['datetime', 'method', 'host', 'status_code', 'req_content_length', 'resp_content_length']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5acab89",
   "metadata": {},
   "source": [
    "## 4. Create Basic HTTP Traffic Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_visualizations(df):\n",
    "    \"\"\"\n",
    "    Create comprehensive HTTP traffic visualizations\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ùå No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìä Creating basic HTTP traffic visualizations...\")\n",
    "    \n",
    "    # Set up the plotting area\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=[\n",
    "            'HTTP Methods Distribution', 'Status Code Distribution',\n",
    "            'Top 10 Hosts by Request Count', 'Request vs Response Size',\n",
    "            'Traffic by Hour of Day', 'Secure vs Non-Secure Traffic'\n",
    "        ],\n",
    "        specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. HTTP Methods Distribution\n",
    "    method_counts = df['method'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=method_counts.index, values=method_counts.values, name=\"Methods\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Status Code Distribution\n",
    "    status_counts = df['status_category'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=status_counts.index, values=status_counts.values, name=\"Status\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Top 10 Hosts\n",
    "    top_hosts = df['host'].value_counts().head(10)\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=top_hosts.values, y=top_hosts.index, orientation='h', name=\"Hosts\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Request vs Response Size\n",
    "    sample_df = df.sample(min(1000, len(df)))  # Sample for performance\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sample_df['req_content_length'],\n",
    "            y=sample_df['resp_content_length'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=sample_df['status_code'],\n",
    "                colorscale='Viridis',\n",
    "                showscale=True,\n",
    "                size=6,\n",
    "                opacity=0.6\n",
    "            ),\n",
    "            name=\"Size Correlation\"\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # 5. Traffic by Hour\n",
    "    hourly_traffic = df.groupby('hour').size().reset_index(name='count')\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=hourly_traffic['hour'], y=hourly_traffic['count'], name=\"Hourly Traffic\"),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # 6. Secure vs Non-Secure\n",
    "    security_counts = df['is_secure'].value_counts()\n",
    "    security_labels = ['HTTPS' if x else 'HTTP' for x in security_counts.index]\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=security_labels, y=security_counts.values, name=\"Security\"),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=1200,\n",
    "        showlegend=False,\n",
    "        title_text=\"HTTP Traffic Analysis Dashboard\",\n",
    "        title_x=0.5\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Create summary statistics\n",
    "    print(f\"\\nüìà Traffic Summary Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Total Requests: {len(df):,}\")\n",
    "    print(f\"   ‚Ä¢ Unique Hosts: {df['host'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Date Range: {df['datetime'].min().date()} to {df['datetime'].max().date()}\")\n",
    "    print(f\"   ‚Ä¢ Most Common Method: {df['method'].mode().iloc[0]} ({df['method'].value_counts().iloc[0]} requests)\")\n",
    "    print(f\"   ‚Ä¢ Most Common Status: {df['status_code'].mode().iloc[0]} ({df['status_code'].value_counts().iloc[0]} responses)\")\n",
    "    print(f\"   ‚Ä¢ Average Request Size: {df['req_content_length'].mean():.2f} bytes\")\n",
    "    print(f\"   ‚Ä¢ Average Response Size: {df['resp_content_length'].mean():.2f} bytes\")\n",
    "    print(f\"   ‚Ä¢ HTTPS Traffic: {(df['is_secure'].sum() / len(df) * 100):.1f}%\")\n",
    "\n",
    "# Create the basic visualizations\n",
    "if not df.empty:\n",
    "    create_basic_visualizations(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d93a8",
   "metadata": {},
   "source": [
    "## 5. Time Series Analysis of Network Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_analysis(df):\n",
    "    \"\"\"\n",
    "    Create comprehensive time series analysis of network traffic\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ùå No data available for time series analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"‚è±Ô∏è Creating time series analysis...\")\n",
    "    \n",
    "    # Create time-based aggregations\n",
    "    df_time = df.set_index('datetime').sort_index()\n",
    "    \n",
    "    # Resample by different time periods\n",
    "    hourly_data = df_time.resample('H').agg({\n",
    "        'method': 'count',\n",
    "        'req_content_length': 'sum',\n",
    "        'resp_content_length': 'sum',\n",
    "        'request_duration': 'mean',\n",
    "        'status_code': lambda x: (x >= 400).sum()  # Error count\n",
    "    }).rename(columns={'method': 'request_count', 'status_code': 'error_count'})\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=1,\n",
    "        subplot_titles=[\n",
    "            'Requests per Hour',\n",
    "            'Data Transfer (Request vs Response)',\n",
    "            'Average Response Time',\n",
    "            'Error Rate Over Time'\n",
    "        ],\n",
    "        vertical_spacing=0.08\n",
    "    )\n",
    "    \n",
    "    # 1. Requests per hour\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hourly_data.index,\n",
    "            y=hourly_data['request_count'],\n",
    "            mode='lines+markers',\n",
    "            name='Requests/Hour',\n",
    "            line=dict(color='blue', width=2)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Data transfer\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hourly_data.index,\n",
    "            y=hourly_data['req_content_length'] / 1024**2,  # Convert to MB\n",
    "            mode='lines',\n",
    "            name='Request Data (MB)',\n",
    "            line=dict(color='green')\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hourly_data.index,\n",
    "            y=hourly_data['resp_content_length'] / 1024**2,  # Convert to MB\n",
    "            mode='lines',\n",
    "            name='Response Data (MB)',\n",
    "            line=dict(color='orange')\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 3. Response time\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hourly_data.index,\n",
    "            y=hourly_data['request_duration'],\n",
    "            mode='lines+markers',\n",
    "            name='Avg Response Time (ms)',\n",
    "            line=dict(color='purple', width=2)\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Error rate\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=hourly_data.index,\n",
    "            y=hourly_data['error_count'],\n",
    "            mode='lines+markers',\n",
    "            name='Errors/Hour',\n",
    "            line=dict(color='red', width=2),\n",
    "            fill='tonexty'\n",
    "        ),\n",
    "        row=4, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=1000,\n",
    "        title_text=\"Time Series Analysis of Network Traffic\",\n",
    "        title_x=0.5,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update x-axes to show time properly\n",
    "    for i in range(1, 5):\n",
    "        fig.update_xaxes(title_text=\"Time\", row=i, col=1)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Request Count\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Data Transfer (MB)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Response Time (ms)\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Error Count\", row=4, col=1)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Traffic pattern analysis\n",
    "    daily_pattern = df.groupby('hour')['method'].count()\n",
    "    peak_hour = daily_pattern.idxmax()\n",
    "    peak_requests = daily_pattern.max()\n",
    "    \n",
    "    weekly_pattern = df.groupby('day_of_week')['method'].count()\n",
    "    busiest_day = weekly_pattern.idxmax()\n",
    "    \n",
    "    print(f\"\\nüìä Traffic Pattern Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Peak Hour: {peak_hour}:00 with {peak_requests} requests\")\n",
    "    print(f\"   ‚Ä¢ Busiest Day: {busiest_day} with {weekly_pattern.max()} requests\")\n",
    "    print(f\"   ‚Ä¢ Total Data Transferred: {(df['req_content_length'].sum() + df['resp_content_length'].sum()) / 1024**3:.2f} GB\")\n",
    "    print(f\"   ‚Ä¢ Average Request Duration: {df['request_duration'].mean():.2f} ms\")\n",
    "    \n",
    "    return hourly_data\n",
    "\n",
    "# Create time series analysis\n",
    "if not df.empty:\n",
    "    hourly_data = create_time_series_analysis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b7accf",
   "metadata": {},
   "source": [
    "## 6. Security and Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def security_analysis(df):\n",
    "    \"\"\"\n",
    "    Perform security-focused analysis and anomaly detection\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ùå No data available for security analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"üîí Performing security analysis and anomaly detection...\")\n",
    "    \n",
    "    # Security indicators\n",
    "    suspicious_indicators = {\n",
    "        'Large Requests': df['is_large_request'].sum(),\n",
    "        'Large Responses': df['is_large_response'].sum(),\n",
    "        'Non-HTTPS Traffic': (~df['is_secure']).sum(),\n",
    "        'Client Errors (4xx)': ((df['status_code'] >= 400) & (df['status_code'] < 500)).sum(),\n",
    "        'Server Errors (5xx)': (df['status_code'] >= 500).sum(),\n",
    "        'Unusual Methods': df[~df['method'].isin(['GET', 'POST', 'PUT', 'DELETE'])]['method'].count()\n",
    "    }\n",
    "    \n",
    "    # Domain analysis\n",
    "    domain_stats = df.groupby('domain').agg({\n",
    "        'method': 'count',\n",
    "        'req_content_length': 'sum',\n",
    "        'resp_content_length': 'sum',\n",
    "        'is_secure': lambda x: x.sum() / len(x)  # HTTPS ratio\n",
    "    }).rename(columns={'method': 'requests', 'is_secure': 'https_ratio'})\n",
    "    \n",
    "    domain_stats = domain_stats.sort_values('requests', ascending=False)\n",
    "    \n",
    "    # Create security dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[\n",
    "            'Security Indicators',\n",
    "            'Top Domains by Request Volume',\n",
    "            'Status Code Distribution',\n",
    "            'HTTPS vs HTTP by Domain (Top 10)'\n",
    "        ],\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"pie\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Security indicators\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=list(suspicious_indicators.keys()),\n",
    "            y=list(suspicious_indicators.values()),\n",
    "            marker_color=['red' if val > 0 else 'green' for val in suspicious_indicators.values()],\n",
    "            name=\"Security Indicators\"\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Top domains\n",
    "    top_domains = domain_stats.head(10)\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=top_domains.index,\n",
    "            y=top_domains['requests'],\n",
    "            name=\"Domain Requests\"\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Status code distribution\n",
    "    status_dist = df['status_code'].value_counts().head(10)\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            labels=[f\"{code}\" for code in status_dist.index],\n",
    "            values=status_dist.values,\n",
    "            name=\"Status Codes\"\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. HTTPS ratio by domain\n",
    "    top_domains_https = domain_stats.head(10)\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=top_domains_https.index,\n",
    "            y=top_domains_https['https_ratio'] * 100,\n",
    "            marker_color=['green' if ratio > 0.8 else 'orange' if ratio > 0.5 else 'red' \n",
    "                         for ratio in top_domains_https['https_ratio']],\n",
    "            name=\"HTTPS %\"\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Security Analysis Dashboard\",\n",
    "        title_x=0.5,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(tickangle=45, row=1, col=1)\n",
    "    fig.update_xaxes(tickangle=45, row=1, col=2)\n",
    "    fig.update_xaxes(tickangle=45, row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Requests\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"HTTPS %\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Anomaly detection using statistical methods\n",
    "    print(f\"\\nüö® Security Analysis Results:\")\n",
    "    print(f\"   ‚Ä¢ Total Security Incidents: {sum(suspicious_indicators.values())}\")\n",
    "    for indicator, count in suspicious_indicators.items():\n",
    "        if count > 0:\n",
    "            print(f\"   ‚Ä¢ {indicator}: {count}\")\n",
    "    \n",
    "    # Find potential data exfiltration (large outbound requests)\n",
    "    large_requests = df[df['req_content_length'] > df['req_content_length'].quantile(0.95)]\n",
    "    if not large_requests.empty:\n",
    "        print(f\"\\nüì§ Potential Data Exfiltration:\")\n",
    "        print(f\"   ‚Ä¢ Large requests detected: {len(large_requests)}\")\n",
    "        print(f\"   ‚Ä¢ Top targets: {large_requests['host'].value_counts().head(3).to_dict()}\")\n",
    "    \n",
    "    # Find unusual user agents\n",
    "    ua_counts = df['user_agent'].value_counts()\n",
    "    rare_uas = ua_counts[ua_counts == 1]\n",
    "    if len(rare_uas) > 0:\n",
    "        print(f\"\\nü§ñ Unusual User Agents:\")\n",
    "        print(f\"   ‚Ä¢ Unique user agents: {len(rare_uas)}\")\n",
    "        print(f\"   ‚Ä¢ Sample: {list(rare_uas.index[:3])}\")\n",
    "    \n",
    "    return domain_stats, suspicious_indicators\n",
    "\n",
    "# Perform security analysis\n",
    "if not df.empty:\n",
    "    domain_stats, security_indicators = security_analysis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b552db24",
   "metadata": {},
   "source": [
    "## 7. Interactive Traffic Filtering and Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bffe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_dashboard(df):\n",
    "    \"\"\"\n",
    "    Create interactive dashboard with filtering capabilities\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ùå No data available for interactive dashboard\")\n",
    "        return\n",
    "    \n",
    "    print(\"üéõÔ∏è Creating interactive dashboard...\")\n",
    "    \n",
    "    # Create filter widgets\n",
    "    date_range = widgets.SelectionRangeSlider(\n",
    "        options=[(date.strftime('%Y-%m-%d'), date) for date in sorted(df['date'].unique())],\n",
    "        index=(0, len(df['date'].unique())-1),\n",
    "        description='Date Range',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    method_filter = widgets.SelectMultiple(\n",
    "        options=df['method'].unique().tolist(),\n",
    "        value=df['method'].unique().tolist(),\n",
    "        description='HTTP Methods',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    host_filter = widgets.Dropdown(\n",
    "        options=['All'] + df['host'].value_counts().head(20).index.tolist(),\n",
    "        value='All',\n",
    "        description='Host Filter',\n",
    "    )\n",
    "    \n",
    "    status_filter = widgets.SelectMultiple(\n",
    "        options=['2xx', '3xx', '4xx', '5xx'],\n",
    "        value=['2xx', '3xx', '4xx', '5xx'],\n",
    "        description='Status Codes',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    search_box = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Search in URLs or hosts...',\n",
    "        description='Search:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def update_dashboard(*args):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Apply filters\n",
    "            filtered_df = df.copy()\n",
    "            \n",
    "            # Date filter\n",
    "            start_date, end_date = date_range.value\n",
    "            filtered_df = filtered_df[\n",
    "                (filtered_df['date'] >= start_date) & \n",
    "                (filtered_df['date'] <= end_date)\n",
    "            ]\n",
    "            \n",
    "            # Method filter\n",
    "            filtered_df = filtered_df[filtered_df['method'].isin(method_filter.value)]\n",
    "            \n",
    "            # Host filter\n",
    "            if host_filter.value != 'All':\n",
    "                filtered_df = filtered_df[filtered_df['host'] == host_filter.value]\n",
    "            \n",
    "            # Status filter\n",
    "            status_conditions = []\n",
    "            for status in status_filter.value:\n",
    "                if status == '2xx':\n",
    "                    status_conditions.append((filtered_df['status_code'] >= 200) & (filtered_df['status_code'] < 300))\n",
    "                elif status == '3xx':\n",
    "                    status_conditions.append((filtered_df['status_code'] >= 300) & (filtered_df['status_code'] < 400))\n",
    "                elif status == '4xx':\n",
    "                    status_conditions.append((filtered_df['status_code'] >= 400) & (filtered_df['status_code'] < 500))\n",
    "                elif status == '5xx':\n",
    "                    status_conditions.append((filtered_df['status_code'] >= 500) & (filtered_df['status_code'] < 600))\n",
    "            \n",
    "            if status_conditions:\n",
    "                status_mask = status_conditions[0]\n",
    "                for condition in status_conditions[1:]:\n",
    "                    status_mask |= condition\n",
    "                filtered_df = filtered_df[status_mask]\n",
    "            \n",
    "            # Search filter\n",
    "            if search_box.value:\n",
    "                search_mask = (\n",
    "                    filtered_df['host'].str.contains(search_box.value, case=False, na=False) |\n",
    "                    filtered_df['path'].str.contains(search_box.value, case=False, na=False) |\n",
    "                    filtered_df['full_url'].str.contains(search_box.value, case=False, na=False)\n",
    "                )\n",
    "                filtered_df = filtered_df[search_mask]\n",
    "            \n",
    "            if filtered_df.empty:\n",
    "                print(\"üîç No data matches the current filters\")\n",
    "                return\n",
    "            \n",
    "            # Create filtered visualizations\n",
    "            print(f\"üìä Filtered Results: {len(filtered_df):,} entries\")\n",
    "            \n",
    "            # Traffic over time\n",
    "            time_series = filtered_df.set_index('datetime').resample('H').size()\n",
    "            \n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=2,\n",
    "                subplot_titles=[\n",
    "                    'Traffic Over Time (Filtered)',\n",
    "                    'Method Distribution (Filtered)',\n",
    "                    'Top Hosts (Filtered)',\n",
    "                    'Status Codes (Filtered)'\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Time series\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=time_series.index, y=time_series.values, mode='lines+markers'),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Methods\n",
    "            method_counts = filtered_df['method'].value_counts()\n",
    "            fig.add_trace(\n",
    "                go.Pie(labels=method_counts.index, values=method_counts.values),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Hosts\n",
    "            host_counts = filtered_df['host'].value_counts().head(10)\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=host_counts.index, y=host_counts.values),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # Status codes\n",
    "            status_counts = filtered_df['status_code'].value_counts().head(10)\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=[str(x) for x in status_counts.index], y=status_counts.values),\n",
    "                row=2, col=2\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(height=600, showlegend=False, title_text=\"Filtered Traffic Analysis\")\n",
    "            fig.update_xaxes(tickangle=45, row=2, col=1)\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "            # Show sample of filtered data\n",
    "            sample_columns = ['datetime', 'method', 'host', 'path', 'status_code', 'req_content_length', 'resp_content_length']\n",
    "            display(HTML(f\\\"<h4>Sample of Filtered Data ({len(filtered_df)} total entries):</h4>\\\"))\n",
    "            display(filtered_df[sample_columns].head(10))\n",
    "    \n",
    "    # Connect widgets to update function\n",
    "    date_range.observe(update_dashboard, names='value')\n",
    "    method_filter.observe(update_dashboard, names='value')\n",
    "    host_filter.observe(update_dashboard, names='value')\n",
    "    status_filter.observe(update_dashboard, names='value')\n",
    "    search_box.observe(update_dashboard, names='value')\n",
    "    \n",
    "    # Display widgets and output\n",
    "    filter_box = widgets.VBox([\n",
    "        widgets.HTML('<h3>üéõÔ∏è Interactive Traffic Filter Dashboard</h3>'),\n",
    "        widgets.HBox([date_range, method_filter]),\n",
    "        widgets.HBox([host_filter, status_filter]),\n",
    "        search_box,\n",
    "        widgets.HTML('<hr>')\n",
    "    ])\n",
    "    \n",
    "    display(filter_box)\n",
    "    display(output)\n",
    "    \n",
    "    # Initial update\n",
    "    update_dashboard()\n",
    "\n",
    "# Create interactive dashboard\n",
    "if not df.empty:\n",
    "    create_interactive_dashboard(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
